@incollection{akaike1998information,
  title     = {Information theory and an extension of the maximum likelihood principle},
  author    = {Akaike, Hirotogu},
  editor    = {Petrov, B. N. and Csaki, F},
  booktitle = {Second International Symposium on Information Theory},
  pages     = {266--281},
  year      = {1973}
}
@Article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Valentin Amrhein and Sander Greenland and Blake McShane},
  date = {2019-03-21},
  journaltitle = {Nature},
  volume = {567},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/d41586-019-00857-9},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  eprint = {30894741},
  eprinttype = {pmid},
    keywords = {Research data,Research management},
  number = {7748},
}
@article{Baird1983,
 ISSN = {00070882, 14643537},
 URL = {http://www.jstor.org/stable/687444},
 author = {Davis Baird},
 journal = {The British Journal for the Philosophy of Science},
 number = {2},
 pages = {105--118},
 publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
 title = {The Fisher/Pearson Chi-Squared Controversy: A Turning Point for Inductive Inference},
 volume = {34},
 year = {1983}
}
@Misc{bayesplot,
  title = {bayesplot: Plotting for Bayesian Models},
  author = {Jonah Gabry and Tristan Mahr},
  year = {2021},
  note = {R package version 1.8.0},
  url = {https://mc-stan.org/bayesplot/},
}
@Article{benjaminRedefineStatisticalSignificance2018,
  title = {Redefine Statistical Significance},
  author = {Daniel J. Benjamin and James O. Berger and Magnus Johannesson and Brian A. Nosek and E.-J. Wagenmakers and Richard Berk and Kenneth A. Bollen and Bj{\"o}rn Brembs and Lawrence Brown and Colin Camerer and David Cesarini and Christopher D. Chambers and Merlise Clyde and Thomas D. Cook and Paul {De Boeck} and Zoltan Dienes and Anna Dreber and Kenny Easwaran and Charles Efferson and Ernst Fehr and Fiona Fidler and Andy P. Field and Malcolm Forster and Edward I. George and Richard Gonzalez and Steven Goodman and Edwin Green and Donald P. Green and Anthony G. Greenwald and Jarrod D. Hadfield and Larry V. Hedges and Leonhard Held and Teck {Hua Ho} and Herbert Hoijtink and Daniel J. Hruschka and Kosuke Imai and Guido Imbens and John P. A. Ioannidis and Minjeong Jeon and James Holland Jones and Michael Kirchler and David Laibson and John List and Roderick Little and Arthur Lupia and Edouard Machery and Scott E. Maxwell and Michael McCarthy and Don A. Moore and Stephen L. Morgan and Marcus Munaf{\a'o} and Shinichi Nakagawa and Brendan Nyhan and Timothy H. Parker and Luis Pericchi and Marco Perugini and Jeff Rouder and Judith Rousseau and Victoria Savalei and Felix D. Sch{\"o}nbrodt and Thomas Sellke and Betsy Sinclair and Dustin Tingley and Trisha {Van Zandt} and Simine Vazire and Duncan J. Watts and Christopher Winship and Robert L. Wolpert and Yu Xie and Cristobal Young and Jonathan Zinman and Valen E. Johnson},
  date = {2018-01-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {6--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  url = {http://www.nature.com/articles/s41562-017-0189-z},
  urldate = {2019-09-08},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
    keywords = {★},
  number = {1},
}
@book{bertsekasIntroductionProbability2nd2008,
  location  = {Belmont, Massachusetts},
  edition   = {2nd edition},
  title     = {Introduction to Probability, 2nd Edition},
  isbn      = {978-1-886529-23-6},
  abstract  = {An intuitive, yet precise introduction to probability theory, stochastic processes, and probabilistic models used in science, engineering, economics, and related fields. The 2nd edition is a substantial revision of the 1st edition, involving a reorganization of old material and the addition of new material. The length of the book has increased by about 25 percent. The main new feature of the 2nd edition is thorough introduction to Bayesian and classical statistics. The book is the currently used textbook for "Probabilistic Systems Analysis," an introductory probability course at the Massachusetts Institute of Technology, attended by a large number of undergraduate and graduate students. The book covers the fundamentals of probability theory (probabilistic models, discrete and continuous random variables, multiple random variables, and limit theorems), which are typically part of a first course on the subject, as well as the fundamental concepts and methods of statistical inference, both Bayesian and classical. It also contains, a number of more advanced topics, from which an instructor can choose to match the goals of a particular course. These topics include transforms, sums of random variables, a fairly detailed introduction to Bernoulli, Poisson, and Markov processes. The book strikes a balance between simplicity in exposition and sophistication in analytical reasoning. Some of the more mathematically rigorous analysis has been just intuitively explained in the text, but is developed in detail (at the level of advanced calculus) in the numerous solved theoretical problems. Written by two professors of the Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology, and members of the prestigious {US} National Academy of Engineering, the book has been widely adopted for classroom use in introductory probability courses within the {USA} and abroad.From a Review of the 1st Edition:...it trains the intuition to acquire probabilistic feeling. This book explains every single concept it enunciates. This is its main strength, deep explanation, and not just examples that happen to explain. Bertsekas and Tsitsiklis leave nothing to chance. The probability to misinterpret a concept or not understand it is just... zero. Numerous examples, figures, and end-of-chapter problems strengthen the understanding. Also of invaluable help is the book's web site, where solutions to the problems can be found-as well as much more information pertaining to probability, and also more problem sets. --Vladimir Botchev, Analog Dialogue Several other reviews can be found in the listing of the first edition of this book. Contents, preface, and more info at publisher's website (Athena Scientific, athenasc com)},
  pagetotal = {544},
  publisher = {Athena Scientific},
  author    = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  date      = {2008-07-15}
}
@article{beskosOptimalTuningHybrid2013,
  title        = {Optimal Tuning of the Hybrid {{Monte Carlo}} Algorithm},
  author       = {Beskos, Alexandros and Pillai, Natesh and Roberts, Gareth and Sanz-Serna, Jesus-Maria and Stuart, Andrew},
  date         = {2013-11},
  journaltitle = {Bernoulli},
  volume       = {19},
  pages        = {1501--1534},
  publisher    = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn         = {1350-7265},
  doi          = {10.3150/12-BEJ414},
  url          = {https://projecteuclid.org/journals/bernoulli/volume-19/issue-5A/Optimal-tuning-of-the-hybrid-Monte-Carlo-algorithm/10.3150/12-BEJ414.full},
  urldate      = {2021-04-30},
  abstract     = {We investigate the properties of the hybrid Monte Carlo algorithm (HMC) in high dimensions. HMC develops a Markov chain reversible with respect to a given target distribution \$\textbackslash Pi\$ using separable Hamiltonian dynamics with potential \$-\textbackslash log\textbackslash Pi\$. The additional momentum variables are chosen at random from the Boltzmann distribution, and the continuous-time Hamiltonian dynamics are then discretised using the leapfrog scheme. The induced bias is removed via a Metropolis–Hastings accept/reject rule. In the simplified scenario of independent, identically distributed components, we prove that, to obtain an \$\textbackslash mathcal\{O\}(1)\$ acceptance probability as the dimension \$d\$ of the state space tends to \$\textbackslash infty\$, the leapfrog step size \$h\$ should be scaled as \$h=l\textbackslash times d\^\{-1/4\}\$. Therefore, in high dimensions, HMC requires \$\textbackslash mathcal\{O\}(d\^\{1/4\})\$ steps to traverse the state space. We also identify analytically the asymptotically optimal acceptance probability, which turns out to be \$0.651\$ (to three decimal places). This value optimally balances the cost of generating a proposal, which decreases as \$l\$ increases (because fewer steps are required to reach the desired final integration time), against the cost related to the average number of proposals required to obtain acceptance, which increases as \$l\$ increases.},
  file         = {/Users/storopoli/Zotero/storage/TFYD5A5Q/Beskos et al. - 2013 - Optimal tuning of the hybrid Monte Carlo algorithm.pdf;/Users/storopoli/Zotero/storage/AQNL3IKP/12-BEJ414.html},
  issue        = {5A},
  keywords     = {Hamiltonian dynamics,high dimensions,leapfrog scheme,optimal acceptance probability,squared jumping distance}
}
@Online{betancourtConceptualIntroductionHamiltonian2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Michael Betancourt},
  date = {2017-01-09},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2019-11-06},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  }
@online{betancourtProbabilisticBuildingBlocks2019,
  title      = {Probabilistic Building Blocks},
  url        = {https://betanalpha.github.io/assets/case_studies/probability_densities.html},
  titleaddon = {Beta \& Alpha},
  author     = {Betancourt, Michael},
  urldate    = {2021-05-27},
  date       = {2019-06}
}
@article{bezanson2017julia,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  year = {2017},
  volume = {59},
  pages = {65--98},
  publisher = {{SIAM}},
    journal = {SIAM review},
  number = {1}
}
@article{boatwright1999account,
  title={Account-level modeling for trade promotion: An application of a constrained parameter hierarchical model},
  author={Boatwright, Peter and McCulloch, Robert and Rossi, Peter},
  journal={Journal of the American Statistical Association},
  volume={94},
  number={448},
  pages={1063--1073},
  year={1999},
  publisher={Taylor \& Francis}
}
@Article{boxScienceStatistics1976,
  title = {Science and {{Statistics}}},
  author = {George E. P. Box},
  date = {1976},
  journaltitle = {Journal of the American Statistical Association},
  volume = {71},
  pages = {791--799},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2286841},
  abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
  eprint = {2286841},
  eprinttype = {jstor},
    number = {356},
}
@Article{brms,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{brooksGeneralMethodsMonitoring1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Stephen P. Brooks and Andrew Gelman},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {null},
  volume = {7},
  pages = {434--455},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
  urldate = {2021-02-07},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
    number = {4},
}
@Book{brooksHandbookMarkovChain2011,
  title = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  date = {2011-05-10},
  publisher = {{CRC Press}},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
  eprint = {qfRsAIKZ4rIC},
  eprinttype = {googlebooks},
  isbn = {978-1-4200-7942-5},
  keywords = {Mathematics / Probability & Statistics / General},
  langid = {english},
  pagetotal = {620},
}
@article{burknerAdvancedBayesianMultilevel2018,
  title        = {Advanced Bayesian Multilevel Modeling with the R Package brms},
  volume       = {10},
  issn         = {2073-4859},
  url          = {https://journal.r-project.org/archive/2018/RJ-2018-017/index.html},
  pages        = {395--411},
  number       = {1},
  journaltitle = {The R Journal},
  author       = {Bürkner, Paul-Christian},
  urldate      = {2021-02-26},
  date         = {2018},
  langid       = {english},
  file         = {Snapshot:/Users/storopoli/Zotero/storage/4GWDN6VA/index.html:text/html;Bürkner - 2018 - Advanced Bayesian Multilevel Modeling with the R P.pdf:/Users/storopoli/Zotero/storage/PXULDGJQ/Bürkner - 2018 - Advanced Bayesian Multilevel Modeling with the R P.pdf:application/pdf}
}
@Article{carpenterStanProbabilisticProgramming2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  publisher = {{American Statistical Association}},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {http://www.jstatsoft.org/v76/i01/},
  urldate = {2019-10-29},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
    keywords = {Algorithmic differentiation,Bayesian inference,Probabilistic program,Stan},
  number = {1},
}
@article{casellaExplainingGibbsSampler1992,
	title = {Explaining the Gibbs Sampler},
	author = {{Casella}, {George} and {George}, {Edward I.}},
	year = {1992},
	month = {08},
	date = {1992-08-01},
	journal = {The American Statistician},
	pages = {167--174},
	volume = {46},
	number = {3},
	doi = {10.1080/00031305.1992.10475878},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475878},
	note = {Publisher: Taylor \& Francis
{\_}eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1992.10475878}
}
@article{chibUnderstandingMetropolisHastingsAlgorithm1995,
  title = {Understanding the {{Metropolis}}-{{Hastings Algorithm}}},
  author = {Chib, Siddhartha and Greenberg, Edward},
  year = {1995},
  month = nov,
  volume = {49},
  pages = {327--335},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1995.10476177},
  journal = {The American Statistician},
  number = {4}
}
@article{cobb2007introductory,
  title={The introductory statistics course: A Ptolemaic curriculum?},
  author={Cobb, George W},
  journal={Technology innovations in statistics education},
  volume={1},
  number={1},
  year={2007}
}
@Article{cohenEarth051994,
  title = {The Earth Is Round (p {$<$} .05)},
  author = {Jacob Cohen},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  pages = {997--1003},
  publisher = {{American Psychological Association Inc.}},
  issn = {0003066X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
    keywords = {★},
  number = {12},
}
@article{cortez2009modeling,
  title={Modeling wine preferences by data mining from physicochemical properties},
  author={Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  journal={Decision support systems},
  volume={47},
  number={4},
  pages={547--553},
  year={2009},
  publisher={Elsevier}
}
@article{cumming2009inference,
  title={Inference by eye: reading the overlap of independent confidence intervals},
  author={Cumming, Geoff},
  journal={Statistics in medicine},
  volume={28},
  number={2},
  pages={205--220},
  year={2009},
  publisher={Wiley Online Library}
}
@Book{definettiTheoryProbability1974,
  title = {Theory of {{Probability}}},
  shorttitle = {Theory of {{Probability}}},
  author = {Bruno {de Finetti}},
  date = {1974},
  edition = {Volume 1},
  publisher = {{John Wiley \& Sons}},
  location = {{New York}},
  abstract = {First issued in translation as a two-volume work in 1975, this classic book~provides the first complete development of the theory of probability from a subjectivist viewpoint. It proceeds from a detailed discussion of the philosophical mathematical aspects to a detailed mathematical treatment of probability and statistics. De Finetti’s theory of probability is one of the foundations of Bayesian theory. De Finetti stated that probability is nothing but a subjective analysis of the likelihood that something will happen and that that probability does not exist outside the mind. ~It is the rate at which a person is willing to bet on something happening. ~This view is directly opposed to the classicist/ frequentist view of the likelihood of a particular outcome of an event, which assumes that the same event could be identically repeated many times over, and the 'probability' of a particular outcome has to do with the fraction of the time that outcome results from the repeated trials.},
  isbn = {978-1-119-28637-0},
  langid = {english},
  options = {useprefix=true},
}
@book{dekkingModernIntroductionProbability2010,
  title      = {A Modern Introduction to Probability and Statistics: Understanding Why and How},
  isbn       = {978-1-84996-952-9},
  shorttitle = {A Modern Introduction to Probability and Statistics},
  abstract   = {Suitable for self study Use real examples and real data sets that will be familiar to the audience  Introduction to the bootstrap is included – this is a modern method missing in many other books},
  pagetotal  = {504},
  publisher  = {Springer},
  author     = {Dekking, F. M. and Kraaikamp, C. and Lopuhaä, H. P. and Meester, L. E.},
  date       = {2010-10-19}
}
@Manual{DescTools,
  title = {{DescTools}: Tools for Descriptive Statistics},
  author = {Signorell {Andri et mult. al.}},
  year = {2021},
  note = {R package version 0.99.40},
  url = {https://cran.r-project.org/package=DescTools},
}
@book{diaconisTenGreatIdeas2019,
  title     = {Ten Great Ideas about Chance},
  isbn      = {978-0-691-19639-8},
  abstract  = {In the sixteenth and seventeenth centuries, gamblers and mathematicians transformed the idea of chance from a mystery into the discipline of probability, setting the stage for a series of breakthroughs that enabled or transformed innumerable fields, from gambling, mathematics, statistics, economics, and finance to physics and computer science. This book tells the story of ten great ideas about chance and the thinkers who developed them, tracing the philosophical implications of these ideas as well as their mathematical impact.},
  pagetotal = {270},
  publisher = {Princeton University Press},
  author    = {Diaconis, Persi and Skyrms, Brian},
  date      = {2019-10-08},
  langid    = {english},
  note      = {Google-Books-{ID}: 68iXDwAAQBAJ},
  keywords  = {Mathematics / General, Mathematics / History \& Philosophy, Mathematics / Probability \& Statistics / General}
}
@Article{dienesBayesianOrthodoxStatistics2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: {{Which Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Zoltan Dienes},
  date = {2011-05-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {6},
  pages = {274--290},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691611406920},
  url = {https://doi.org/10.1177/1745691611406920},
  urldate = {2021-02-04},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing—two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
    keywords = {Bayes,evidence,likelihood principle,significance testing,statistical inference},
  langid = {english},
  number = {3},
}
@manual{downey2016,
 title  = "Probably Overthinking It: There is still only one test",
 author = "Allen Downey",
 url    = "http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html",
 year   = "2016"
}
@Article{duaneHybridMonteCarlo1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Simon Duane and A. D. Kennedy and Brian J. Pendleton and Duncan Roweth},
  date = {1987-09-03},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  volume = {195},
  pages = {216--222},
  issn = {0370-2693},
  doi = {10.1016/0370-2693(87)91197-X},
  url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
  urldate = {2021-02-07},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
    langid = {english},
  number = {2},
}
@article{duncan1961socioeconomic,
  title={A socioeconomic index for all occupations},
  author={Duncan, Otis Dudley},
  journal={Class: Critical Concepts},
  volume={1},
  pages={388--426},
  year={1961},
  publisher={Rutledge New York, NY}
}
@article{eckhardtStanUlamJohn1987,
  title = {Stan {{Ulam}}, {{John}} von {{Neumann}}, and the {{Monte Carlo Method}}},
  author = {Eckhardt, Roger},
  year = {1987},
  volume = {15},
  pages = {131--136},
    journal = {Los Alamos Science},
  number = {30}
}

@Article{etzHowBecomeBayesian2018,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Alexander Etz and Quentin F. Gronau and Fabian Dablander and Peter A. Edelsbrunner and Beth Baribault},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {219--234},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1317-5},
  url = {https://doi.org/10.3758/s13423-017-1317-5},
  urldate = {2021-02-04},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to gain background knowledge about various theoretical specifics and Bayesian approaches to frequently used models. Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment. After consulting our guide, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.},
    langid = {english},
  number = {1},
}
@Article{etzIntroductionBayesianInference2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Alexander Etz and Joachim Vandekerckhove},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {5--34},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  url = {https://doi.org/10.3758/s13423-017-1262-3},
  urldate = {2021-02-04},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes’ rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
    langid = {english},
  number = {1},
}
@Article{etzIntroductionConceptLikelihood2018,
  title = {Introduction to the {{Concept}} of {{Likelihood}} and {{Its Applications}}},
  author = {Alexander Etz},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {60--69},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917744314},
  url = {https://doi.org/10.1177/2515245917744314},
  urldate = {2021-02-04},
  abstract = {This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
    keywords = {Bayes factor,Bayesian,estimation,frequentist,likelihood ratio,tutorial},
  langid = {english},
  number = {1},
}
@book{fisher1925statistical,
  title={Statistical methods for research workers},
  author={Fisher, Ronald Aylmer},
  year={1925},
  publisher={Oliver and Boyd}
}
@Article{fisherExamplesBayesMethod1962,
  title = {Some {{Examples}} of {{Bayes}}' {{Method}} of the {{Experimental Determination}} of {{Probabilities A Priori}}},
  author = {Fisher, Ronald Aylmer},
  date = {1962},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {24},
  pages = {118--124},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {Some further examples are given of Bayes' method of determining probabilities a priori by an experiment.},
  eprint = {2983751},
  eprinttype = {jstor},
    number = {1},
}
@Article{gabryVisualizationBayesianWorkflow2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Jonah Gabry and Daniel Simpson and Aki Vehtari and Michael Betancourt and Andrew Gelman},
  date = {2019-02-01},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  pages = {389--402},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {09641998},
  doi = {10.1111/rssa.12378},
  url = {http://doi.wiley.com/10.1111/rssa.12378},
  urldate = {2019-09-16},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
    keywords = {★,Bayesian data analysis,bayesplot,Statistical graphics,Statistical workflow},
  number = {2},
}
@Manual{gapminder,
  title = {gapminder: Data from Gapminder},
  author = {Jennifer Bryan},
  year = {2017},
  note = {R package version 0.3.0},
  url = {https://CRAN.R-project.org/package=gapminder},
}
@article{geisser1979predictive,
  title={A predictive approach to model selection},
  author={Geisser, Seymour and Eddy, William F},
  journal={Journal of the American Statistical Association},
  volume={74},
  number={365},
  pages={153--160},
  year={1979},
  publisher={Taylor \& Francis Group}
}
@incollection{gelfand1992model,
  title     = {Model determination using predictive distributions with implementation via sampling-based methods},
  author    = {Gelfand, A. E. and Dey, D. K. and Chang, H.},
  editor    = {Bernardo, J. M. and Berger, J. O. and Dawid, A. P. and Smith, A. F. M.},
  booktitle = {Bayesian Statistics},
  pages     = {147--167},
  year      = {1992},
  publisher = {Oxford University Press}
}
@article{gelfand1996model,
  title={Model determination using sampling-based methods},
  author={Gelfand, Alan E},
  journal={Markov chain Monte Carlo in practice},
  pages={145--161},
  year={1996},
  publisher={London}
}
@Book{gelman2013bayesian,
  title = {Bayesian {{Data Analysis}}},
  author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
  year = {2013},
  publisher = {{Chapman and Hall/CRC}},
}
@Book{gelman2020regression,
  title = {Regression and Other Stories},
  author = {Andrew Gelman and Jennifer Hill and Aki Vehtari},
  year = {2020},
  publisher = {{Cambridge University Press}},
}
@InCollection{gelmanBasicsMarkovChain2013,
  title = {Basics of {{Markov Chain Simulation}}},
  booktitle = {Bayesian {{Data Analysis}}},
  author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
  date = {2013},
  publisher = {{Chapman and Hall/CRC}},
}
@Online{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian B{\"u}rkner and Martin Modr{\a'a}k},
  date = {2020-11-03},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2021-02-04},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  eprint = {2011.01808},
  eprinttype = {arxiv},
    keywords = {Statistics - Methodology},
  primaryclass = {stat},
}
@Book{gelmanDataAnalysisUsing2007,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Andrew Gelman and Jennifer Hill},
  date = {2007},
  publisher = {{Cambridge university press}},
  }
@Online{gelmanFolkTheoremStatistical2008,
  title = {The Folk Theorem of Statistical Computing},
  author = {Andrew Gelman},
  date = {2008},
  url = {https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/},
  }
@Article{gelmanInferenceIterativeSimulation1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Andrew Gelman and Donald B. Rubin},
  date = {1992-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {7},
  pages = {457--472},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011136},
  url = {https://projecteuclid.org/euclid.ss/1177011136},
  urldate = {2021-02-07},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
    keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  langid = {english},
  number = {4},
  zmnumber = {06853057},
}
@InProceedings{gelmanIterativeNonIterativeSimulation1992,
  title = {Iterative and {{Non}}-{{Iterative Simulation Algorithms}}},
  booktitle = {Computing {{Science}} and {{Statistics}} ({{Interface Proceedings}})},
  author = {Andrew Gelman},
  date = {1992},
  volume = {24},
  pages = {457--511},
  publisher = {{PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS}},
}
@Article{gemanStochasticRelaxationGibbs1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {S. Geman and D. Geman},
  date = {1984-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
    keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution},
  number = {6},
}
@inproceedings{geTuringLanguageFlexible2018,
  title = {Turing: {{A Language}} for {{Flexible Probabilistic Inference}}},
  shorttitle = {Turing},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  year = {2018},
  month = mar,
  pages = {1682--1690},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference eng...},
    language = {en}
}
@InCollection{geyer2011introduction,
  title = {Introduction to Markov Chain Monte Carlo},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Charles J Geyer},
  editor = {Steve Brooks and Andrew Gelman and Galin L. Jones and Xiao-Li Meng},
  date = {2011},
  }
@Article{Goodman1180,
  title = {Aligning Statistical and Scientific Reasoning},
  author = {Steven N Goodman},
  date = {2016},
  journaltitle = {Science},
  volume = {352},
  pages = {1180--1181},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aaf5406},
  url = {https://science.sciencemag.org/content/352/6290/1180},
  annotation = {59 citations (Semantic Scholar/DOI) [2021-02-13]},
    number = {6290},
}
@Manual{gtsummary,
  title = {gtsummary: Presentation-Ready Data Summary and Analytic Result Tables},
  author = {Daniel D. Sjoberg and Michael Curry and Margie Hannum and Joseph Larmarange and Karissa Whiting and Emily C. Zabor},
  year = {2021},
  note = {https://github.com/ddsjoberg/gtsummary,
http://www.danieldsjoberg.com/gtsummary/},
}
@Article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {W. K. Hastings},
  date = {1970-04-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {57},
  pages = {97--109},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.1.97},
  url = {https://doi.org/10.1093/biomet/57.1.97},
  urldate = {2021-02-07},
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
    number = {1},
}
@article{head2015extent,
  title={The extent and consequences of p-hacking in science},
  author={Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
  journal={PLoS Biol},
  volume={13},
  number={3},
  pages={e1002106},
  year={2015},
  publisher={Public Library of Science}
}
@article{Hoekstra2014,
author = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-013-0572-3},
issn = {1531-5320},
journal = {Psychonomic Bulletin {\&} Review},
mendeley-groups = {Statistics Bayesian},
number = {5},
pages = {1157--1164},
title = {{Robust misinterpretation of confidence intervals}},
url = {https://doi.org/10.3758/s13423-013-0572-3},
volume = {21},
year = {2014}
}
@Article{hoffman2014no,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  author = {Matthew D Hoffman and Andrew Gelman},
  date = {2011-11-17},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1593--1623},
  url = {http://arxiv.org/abs/1111.4246},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient {"}turnkey{"} sampling algorithms.},
  archiveprefix = {arXiv},
  eprint = {1111.4246},
  eprinttype = {arxiv},
    number = {1},
}
@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}
@article{Ioannidis2019,
author = {Ioannidis, John P. A.},
doi = {10.1080/00031305.2018.1447512},
issn = {0003-1305},
journal = {The American Statistician},
number = {sup1},
pages = {20--25},
title = {{What Have We (Not) Learnt from Millions of Scientific Papers with {\textless}i{\textgreater}P{\textless}/i{\textgreater} Values?}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1447512},
volume = {73},
year = {2019}
}
@book{irseles2008numericalanalysis,
  author    = {Iserles, Arieh},
  title     = {A First Course in the Numerical Analysis of Differential Equations},
  year      = {2008},
  isbn      = {0521734908},
  publisher = {Cambridge University Press},
  address   = {USA},
  edition   = {2nd},
  abstract  = {Numerical analysis presents different faces to the world. For mathematicians it is a bona fide mathematical theory with an applicable flavour. For scientists and engineers it is a practical, applied subject, part of the standard repertoire of modelling techniques. For computer scientists it is a theory on the interplay of computer architecture and algorithms for real-number calculations. The tension between these standpoints is the driving force of this book, which presents a rigorous account of the fundamentals of numerical analysis of both ordinary and partial differential equations. The exposition maintains a balance between theoretical, algorithmic and applied aspects. This new edition has been extensively updated, and includes new chapters on emerging subject areas: geometric numerical integration, spectral methods and conjugate gradients. Other topics covered include multistep and Runge-Kutta methods; finite difference and finite elements techniques for the Poisson equation; and a variety of algorithms to solve large, sparse algebraic systems.}
}
@Article{ItTimeTalk2019,
  title = {It’s Time to Talk about Ditching Statistical Significance},
  date = {2019-03-20},
  journaltitle = {Nature},
  volume = {567},
  pages = {283--283},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-00874-8},
  url = {https://www.nature.com/articles/d41586-019-00874-8},
  urldate = {2021-03-30},
  abstract = {Looking beyond a much used and abused measure would make science harder, but better.},
    issue = {7748},
  langid = {english},
  number = {7748},
}
@Book{jaynesProbabilityTheoryLogic2003,
  title = {Probability Theory: {{The}} Logic of Science},
  shorttitle = {Probability Theory},
  author = {Jaynes, Edwin T.},
  date = {2003},
  publisher = {{Cambridge university press}},
  }

@Article{junior2020vale,
  title = {Quanto Vale o Valor-p?},
  author = {Carlos Alberto Mour{\~a}o J{\a'u}nior},
  date = {2020},
  journaltitle = {Arquivos de Ciências do Esporte},
  volume = {7},
  number = {2},
}
@Article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Norbert L. Kerr},
  date = {1998},
  journaltitle = {Personality and Social Psychology Review},
  volume = {2},
  pages = {196--217},
  publisher = {{SAGE Publications Inc.}},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
    keywords = {★},
  number = {3},
}
@online{khanBayesianLearningRule2021,
  title         = {The {{Bayesian Learning Rule}}},
  author        = {Khan, Mohammad Emtiyaz and Rue, Håvard},
  date          = {2021-07-09},
  eprint        = {2107.04562},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/2107.04562},
  urldate       = {2021-07-13},
  abstract      = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
  archiveprefix = {arXiv},
  keywords      = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/home/storopoli/Zotero/storage/HRVWVE4W/Khan and Rue - 2021 - The Bayesian Learning Rule.pdf;/home/storopoli/Zotero/storage/VQDN5PLN/2107.html}
}
@Book{kolmogorovFoundationsTheoryProbability1933,
  title = {Foundations of the {{Theory}} of {{Probability}}},
  shorttitle = {Foundations of the {{Theory}} of {{Probability}}},
  author = {Andrey Nikolaevich Kolmogorov},
  date = {1933},
  publisher = {{Julius Springer}},
  location = {{Berlin}},
  }
@InCollection{kruschke2015bayesian,
  title = {Bayesian Estimation in Hierarchical Models},
  booktitle = {The {{Oxford}} Handbook of Computational and Mathematical Psychology},
  author = {John K Kruschke and Wolf Vanpaemel},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {279--299},
  publisher = {{Oxford University Press Oxford, UK}},
    isbn = {978-0-19-995799-6},
  keywords = {★},
}
@Article{kruschkeBayesianDataAnalysis2018,
  title = {Bayesian Data Analysis for Newcomers},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {155--177},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1272-1},
  url = {https://doi.org/10.3758/s13423-017-1272-1},
  urldate = {2021-02-04},
  abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
    langid = {english},
  number = {1},
}
@Article{kruschkeBayesianNewStatistics2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {178--206},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  url = {https://doi.org/10.3758/s13423-016-1221-4},
  urldate = {2021-02-04},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed “the New Statistics” (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
    langid = {english},
  number = {1},
}
@book{kurtBayesianStatisticsFun2019,
  location   = {San Francisco},
  edition    = {Illustrated edition},
  title      = {Bayesian Statistics the Fun Way: Understanding Statistics and Probability with Star Wars, {LEGO}, and Rubber Ducks},
  isbn       = {978-1-59327-956-1},
  shorttitle = {Bayesian Statistics the Fun Way},
  abstract   = {Fun guide to learning Bayesian statistics and probability through unusual and illustrative examples.Probability and statistics are increasingly important in a huge range of professions. But many people use data in ways they don't even understand, meaning they aren't getting the most from it. Bayesian Statistics the Fun Way will change that.This book will give you a complete understanding of Bayesian statistics through simple explanations and un-boring examples. Find out the probability of {UFOs} landing in your garden, how likely Han Solo is to survive a flight through an asteroid shower, how to win an argument about conspiracy theories, and whether a burglary really was a burglary, to name a few examples.By using these off-the-beaten-track examples, the author actually makes learning statistics fun. And you'll learn real skills, like how to:- How to measure your own level of uncertainty in a conclusion or belief- Calculate Bayes theorem and understand what it's useful for- Find the posterior, likelihood, and prior to check the accuracy of your conclusions- Calculate distributions to see the range of your data- Compare hypotheses and draw reliable conclusions from {themNext} time you find yourself with a sheaf of survey results and no idea what to do with them, turn to Bayesian Statistics the Fun Way to get the most value from your data.},
  pagetotal  = {256},
  publisher  = {No Starch Press},
  author     = {Kurt, Will},
  date       = {2019-07-09}
}
@Article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Daniel Lakens and Federico G. Adolfi and Casper J. Albers and Farid Anvari and Matthew A.J. Apps and Shlomo E. Argamon and Thom Baguley and Raymond B. Becker and Stephen D. Benning and Daniel E. Bradford and Erin M. Buchanan and Aaron R. Caldwell and Ben {Van Calster} and Rickard Carlsson and Sau Chin Chen and Bryan Chung and Lincoln J. Colling and Gary S. Collins and Zander Crook and Emily S. Cross and Sameera Daniels and Henrik Danielsson and Lisa Debruine and Daniel J. Dunleavy and Brian D. Earp and Michele I. Feist and Jason D. Ferrell and James G. Field and Nicholas W. Fox and Amanda Friesen and Caio Gomes and Monica Gonzalez-Marquez and James A. Grange and Andrew P. Grieve and Robert Guggenberger and James Grist and Anne Laura {Van Harmelen} and Fred Hasselman and Kevin D. Hochard and Mark R. Hoffarth and Nicholas P. Holmes and Michael Ingre and Peder M. Isager and Hanna K. Isotalus and Christer Johansson and Konrad Juszczyk and David A. Kenny and Ahmed A. Khalil and Barbara Konat and Junpeng Lao and Erik Gahner Larsen and Gerine M.A. Lodder and Ji{\v r}{\a'\i} Lukavsk{\a`y} and Christopher R. Madan and David Manheim and Stephen R. Martin and Andrea E. Martin and Deborah G. Mayo and Randy J. McCarthy and Kevin McConway and Colin McFarland and Amanda Q.X. Nio and Gustav Nilsonne and Cilene Lino {De Oliveira} and Jean Jacques Orban {De Xivry} and Sam Parsons and Gerit Pfuhl and Kimberly A. Quinn and John J. Sakon and S. Adil Saribay and Iris K. Schneider and Manojkumar Selvaraju and Zsuzsika Sjoerds and Samuel G. Smith and Tim Smits and Jeffrey R. Spies and Vishnu Sreekumar and Crystal N. Steltenpohl and Neil Stenhouse and Wojciech {{\a'S}wi{\k a}tkowski} and Miguel A. Vadillo and Marcel A.L.M. {Van Assen} and Matt N. Williams and Samantha E. Williams and Donald R. Williams and Tal Yarkoni and Ignazio Ziano and Rolf A. Zwaan},
  date = {2018-03-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {23973374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
    keywords = {★},
  number = {3},
}
@article{lewandowski2009generating,
  title={Generating random correlation matrices based on vines and extended onion method},
  author={Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  journal={Journal of multivariate analysis},
  volume={100},
  number={9},
  pages={1989--2001},
  year={2009},
  publisher={Elsevier}
}
@Article{lme4,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
}
@Misc{loo,
  title = {loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models},
  author = {Aki Vehtari and Jonah Gabry and Mans Magnusson and Yuling Yao and Paul-Christian Bürkner and Topi Paananen and Andrew Gelman},
  year = {2020},
  note = {R package version 2.4.1},
  url = {https://mc-stan.org/loo/},
}
@Book{mcelreath2020statistical,
  title = {Statistical Rethinking: {{A Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  author = {Richard McElreath},
  year = {2020},
  publisher = {{CRC press}},
}
@Article{mcshaneAbandonStatisticalSignificance2019,
  title = {Abandon {{Statistical Significance}}},
  author = {Blakeley B. McShane and David Gal and Andrew Gelman and Christian Robert and Jennifer L. Tackett},
  date = {2019-03-29},
  journaltitle = {American Statistician},
  volume = {73},
  pages = {235--245},
  publisher = {{American Statistical Association}},
  issn = {15372731},
  doi = {10.1080/00031305.2018.1527253},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm--and the p-value thresholds intrinsic to it--as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to {"}ban{"} p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
    issue = {sup1},
  keywords = {★,Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
}
@Article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Nicholas Metropolis and Arianna W. Rosenbluth and Marshall N. Rosenbluth and Augusta H. Teller and Edward Teller},
  date = {1953-06-01},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {J. Chem. Phys.},
  volume = {21},
  pages = {1087--1092},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  url = {https://aip.scitation.org/doi/abs/10.1063/1.1699114},
  urldate = {2021-02-07},
    number = {6},
}
@article{Morey2016,
author = {Morey, Richard D. and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D. and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-015-0947-8},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Statistical inference,Statistics},
mendeley-groups = {Statistics Bayesian},
month = {feb},
number = {1},
pages = {103--123},
publisher = {Springer New York LLC},
title = {{The fallacy of placing confidence in confidence intervals}},
url = {http://link.springer.com/10.3758/s13423-015-0947-8},
volume = {23},
year = {2016}
}
@Article{murphyHARKingHowBadly2019,
  title = {{{HARKing}}: {{How Badly Can Cherry}}-{{Picking}} and {{Question Trolling Produce Bias}} in {{Published Results}}?},
  author = {Kevin R. Murphy and Herman Aguinis},
  date = {2019-02-15},
  journaltitle = {Journal of Business and Psychology},
  volume = {34},
  publisher = {{Springer New York LLC}},
  issn = {08893268},
  doi = {10.1007/s10869-017-9524-7},
  abstract = {© 2017 Springer Science+Business Media, LLC, part of Springer Nature The practice of hypothesizing after results are known (HARKing) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of HARKing behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When HARKing involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, HARKing has only a small effect on estimates of the population effect size. When HARKing involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, HARKing produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of HARKing behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of HARKing in future research.},
    keywords = {★,Data snooping,HARKing,Publication bias,Simulation},
  number = {1},
}
@article{muth2018user,
  title        = {User-friendly Bayesian regression modeling: A tutorial with rstanarm and shinystan},
  volume       = {14},
  pages        = {99--119},
  number       = {2},
  journaltitle = {Quantitative Methods for Psychology},
  author       = {Muth, Chelsea and Oravecz, Zita and Gabry, Jonah},
  date         = {2018},
  file         = {PDF:/Users/storopoli/Zotero/storage/ES42RA92/Muth, Oravecz, Gabry - 2018 - User-friendly Bayesian regression modeling A tutorial with rstanarm and shinystan.pdf:application/pdf}
}
@Article{nauFinettiWasRight2001,
  title = {De {{Finetti}} Was {{Right}}: {{Probability Does Not Exist}}},
  shorttitle = {De {{Finetti}} Was {{Right}}},
  author = {Robert F. Nau},
  date = {2001-12-01},
  journaltitle = {Theory and Decision},
  shortjournal = {Theory and Decision},
  volume = {51},
  pages = {89--124},
  issn = {1573-7187},
  doi = {10.1023/A:1015525808214},
  url = {https://doi.org/10.1023/A:1015525808214},
  urldate = {2021-02-07},
  abstract = {De Finetti's treatise on the theory of probability begins with the provocative statement PROBABILITY DOES NOT EXIST, meaning that probability does not exist in an objective sense. Rather, probability exists only subjectively within the minds of individuals. De Finetti defined subjective probabilities in terms of the rates at which individuals are willing to bet money on events, even though, in principle, such betting rates could depend on state-dependent marginal utility for money as well as on beliefs. Most later authors, from Savage onward, have attempted to disentangle beliefs from values by introducing hypothetical bets whose payoffs are abstract consequences that are assumed to have state-independent utility. In this paper, I argue that de Finetti was right all along: PROBABILITY, considered as a numerical measure of pure belief uncontaminated by attitudes toward money, does not exist. Rather, what exist are de Finetti's `previsions', or betting rates for money, otherwise known in the literature as `risk neutral probabilities'. But the fact that previsions are not measures of pure belief turns out not to be problematic for statistical inference, decision analysis, or economic modeling.},
  annotation = {80 citations (Semantic Scholar/DOI) [2021-02-13]},
    langid = {english},
  number = {2},
}
@InCollection{neal2011mcmc,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Radford M Neal},
  editor = {Steve Brooks and Andrew Gelman and Galin L. Jones and Xiao-Li Meng},
  date = {2011},
  }
@Article{nealImprovedAcceptanceProcedure1994,
  title = {An {{Improved Acceptance Procedure}} for the {{Hybrid Monte Carlo Algorithm}}},
  author = {Radford M. Neal},
  date = {1994-03-01},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {111},
  pages = {194--203},
  issn = {0021-9991},
  doi = {10.1006/jcph.1994.1054},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
  urldate = {2021-02-07},
  abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted.},
    langid = {english},
  number = {1},
}
@Article{nealSliceSampling2003,
  title = {Slice {{Sampling}}},
  author = {Radford M. Neal},
  date = {2003},
  journaltitle = {The Annals of Statistics},
  volume = {31},
  pages = {705--741},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal {"}slice{"} defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such {"}slice sampling{"} methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by {"}overrelaxation,{"} and for multivariate slice sampling by {"}reflection{"} from the edges of the slice.},
  eprint = {3448413},
  eprinttype = {jstor},
  number = {3},
}
@article{nesterov2009primal,
  title     = {Primal-dual subgradient methods for convex problems},
  author    = {Nesterov, Yurii},
  journal   = {Mathematical programming},
  volume    = {120},
  number    = {1},
  pages     = {221--259},
  year      = {2009},
  publisher = {Springer}
}
@article{neyman1933,
  title={On the problem of the most efficient tests of statistical hypotheses},
  author={Neyman, Jerzy and Pearson, Egon Sharpe},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume={231},
  number={694-706},
  pages={289--337},
  year={1933},
  publisher={The Royal Society London}
}
@article{neyman1937outline,
  title={Outline of a theory of statistical estimation based on the classical theory of probability},
  author={Neyman, Jerzy},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
  volume={236},
  number={767},
  pages={333--380},
  year={1937},
  publisher={The Royal Society London}
}
@article{perkelJuliaComeSyntax2019,
  title = {Julia: Come for the Syntax, Stay for the Speed},
  shorttitle = {Julia},
  author = {Perkel, Jeffrey M.},
  year = {2019},
  month = jul,
  volume = {572},
  pages = {141--142},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-02310-3},
  abstract = {Researchers often find themselves coding algorithms in one programming language, only to have to rewrite them in a faster one. An up-and-coming language could be the answer.},
  copyright = {2021 Nature},
    journal = {Nature},
  language = {en},
  number = {7767}
}
@Manual{postcards,
  title = {postcards: Create Beautiful, Simple Personal Websites},
  author = {Sean Kross},
  year = {2021},
  note = {R package version 0.2.0},
  url = {https://CRAN.R-project.org/package=postcards},
}
@article{pymc3,
  title={Probabilistic programming in Python using PyMC3},
  author={Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
  journal={PeerJ Computer Science},
  volume={2},
  pages={e55},
  year={2016},
  publisher={PeerJ Inc.}
}
@Manual{readxl,
  title = {readxl: Read Excel Files},
  author = {Hadley Wickham and Jennifer Bryan},
  year = {2019},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=readxl},
}
@book{ritchie2020science,
  title={Science fictions: Exposing fraud, bias, negligence and hype in science},
  author={Ritchie, Stuart},
  year={2020},
  publisher={Random House}
}
@Article{robertsWeakConvergenceOptimal1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {G. O. Roberts and A. Gelman and W. R. Gilks},
  date = {1997-02},
  journaltitle = {Annals of Applied Probability},
  shortjournal = {Ann. Appl. Probab.},
  volume = {7},
  pages = {110--120},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/aoap/1034625254},
  url = {https://projecteuclid.org/euclid.aoap/1034625254},
  urldate = {2021-02-07},
  abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞∞\textbackslash infty. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
    keywords = {Markov chain Monte Carlo,Metropolis algorithm,optimal scaling,weak convergence},
  langid = {english},
  mrnumber = {MR1428751},
  number = {1},
  zmnumber = {0876.60015},
}
@article{rosnow1989statistical,
  title={Statistical procedures and the justification of knowledge in psychological science},
  author={Rosnow, Ralph L and Rosenthal, Robert},
  journal={American Psychologist},
  volume={44},
  pages={1276--1284},
  year={1989},
  publisher={American Psychological Association (PsycARTICLES)}
}
@Misc{rstanarm,
  title = {rstanarm: {Bayesian} applied regression modeling via {Stan}.},
  author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam Brilleman},
  note = {R package version 2.21.1},
  year = {2020},
  url = {https://mc-stan.org/rstanarm},
}
@inproceedings{saculinggan2013empirical,
  title={Empirical power comparison of goodness of fit tests for normality in the presence of outliers},
  author={Saculinggan, Mayette and Balase, Emily Amor},
  booktitle={Journal of Physics: Conference Series},
  volume={435},
  number={1},
  pages={012041},
  year={2013},
  organization={IOP Publishing}
}
@Article{schootGentleIntroductionBayesian2014,
  title = {A {{Gentle Introduction}} to {{Bayesian Analysis}}: {{Applications}} to {{Developmental Research}}},
  shorttitle = {A {{Gentle Introduction}} to {{Bayesian Analysis}}},
  author = {Rens {van de Schoot} and David Kaplan and Jaap Denissen and Jens B. Asendorpf and Franz J. Neyer and Marcel A. G. {van Aken}},
  date = {2014},
  journaltitle = {Child Development},
  volume = {85},
  pages = {842--860},
  issn = {1467-8624},
  doi = {10.1111/cdev.12169},
  url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/cdev.12169},
  urldate = {2021-02-04},
  abstract = {Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169},
    langid = {english},
  number = {3},
}
@Manual{skimr,
  title = {skimr: Compact and Flexible Summaries of Data},
  author = {Elin Waring and Michael Quinn and Amelia McNamara and Eduardo {Arino de la Rubia} and Hao Zhu and Shannon Ellis},
  year = {2021},
  note = {R package version 2.1.3},
  url = {https://CRAN.R-project.org/package=skimr},
}
@article{spiegelhalter2002bayesian,
  title={Bayesian measures of model complexity and fit},
  author={Spiegelhalter, David J and Best, Nicola G and Carlin, Bradley P and Van Der Linde, Angelika},
  journal={Journal of the royal statistical society: Series b (statistical methodology)},
  volume={64},
  number={4},
  pages={583--639},
  year={2002},
  publisher={Wiley Online Library}
}
@Article{starkCargocultStatisticsScientific2018,
  title = {Cargo-Cult Statistics and Scientific Crisis},
  author = {Philip B. Stark and Andrea Saltelli},
  date = {2018-08-01},
  journaltitle = {Significance},
  volume = {15},
  pages = {40--43},
  publisher = {{John Wiley \& Sons, Ltd (10.1111)}},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2018.01174.x},
  url = {http://doi.wiley.com/10.1111/j.1740-9713.2018.01174.x},
  urldate = {2019-08-01},
    keywords = {★},
  number = {4},
}
@article{stigler2007epic,
  title={The epic story of maximum likelihood},
  author={Stigler, Stephen M and others},
  journal={Statistical Science},
  volume={22},
  number={4},
  pages={598--620},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}
@misc{storopoli2021bayesianjulia,
  author = {Storopoli, Jose},
  title = {Bayesian Statistics with Julia and Turing},
  url = {https://storopoli.io/Bayesian-Julia},
  year = {2021}
}
@misc{storopoli2021estatisticabayesianaR,
  author = {Storopoli, Jose},
  title = {Estatística Bayesiana com R e Stan},
  url = {https://storopoli.io/Estatistica-Bayesiana},
  year = {2021}
}
@misc{storopoli2022bayesian,
  author = {Storopoli, Jose},
  title = {Bayesian Statistics: a graduate course},
  url = {https://github.com/storopoli/Bayesian-Statistics},
  year = {2022}
}
@article{van2005dic,
  title={DIC in variable selection},
  author={Van Der Linde, Angelika},
  journal={Statistica Neerlandica},
  volume={59},
  number={1},
  pages={45--56},
  year={2005},
  publisher={Wiley Online Library}
}
@InCollection{vandekerckhove2015model,
  title = {Model Comparison and the Principle of Parsimony},
  booktitle = {Oxford Handbook of Computational and Mathematical Psychology},
  author = {Joachim Vandekerckhove and Dora Matzke and Eric-Jan Wagenmakers and {others}},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {300--319},
  publisher = {{Oxford University Press Oxford}},
    isbn = {978-0-19-995799-6},
  keywords = {★},
}
@Article{vandeschootBayesianStatisticsModelling2021,
  title = {Bayesian Statistics and Modelling},
  author = {Rens {van de Schoot} and Sarah Depaoli and Ruth King and Bianca Kramer and Kaspar M{\"a}rtens and Mahlet G. Tadesse and Marina Vannucci and Andrew Gelman and Duco Veen and Joukje Willemsen and Christopher Yau},
  date = {2021-01-14},
  journaltitle = {Nature Reviews Methods Primers},
  volume = {1},
  pages = {1--26},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {https://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2021-02-15},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
    issue = {1},
  langid = {english},
  number = {1},
  options = {useprefix=true},
}
@Article{vanravenzwaaijSimpleIntroductionMarkov2018,
  title = {A Simple Introduction to {{Markov Chain Monte}}–{{Carlo}} Sampling},
  author = {Don {van Ravenzwaaij} and Pete Cassey and Scott D. Brown},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin and Review},
  volume = {25},
  pages = {143--154},
  publisher = {{Springer New York LLC}},
  issn = {15315320},
  doi = {10.3758/s13423-016-1015-8},
  abstract = {© 2016, The Author(s). Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
    keywords = {★,Bayesian inference,Markov Chain Monte–Carlo,MCMC,Tutorial},
  number = {1},
  options = {useprefix=true},
}
@Online{vehtariPracticalBayesianModel2015,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Aki Vehtari and Andrew Gelman and Jonah Gabry},
  date = {2015-07-16},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://arxiv.org/abs/1507.04544},
  urldate = {2019-11-06},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  annotation = {1221 citations (Semantic Scholar/DOI) [2021-02-13] 1221 citations (Semantic Scholar/arXiv) [2021-02-13]},
  archiveprefix = {arXiv},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  }
@Manual{vitae,
  title = {vitae: Curriculum Vitae for R Markdown},
  author = {Mitchell O'Hara-Wild and Rob Hyndman},
  year = {2021},
  note = {R package version 0.4.1},
  url = {https://CRAN.R-project.org/package=vitae},
}
@Article{Wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Eric-Jan Wagenmakers},
  date = {2007-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {14},
  pages = {779--804},
  publisher = {{Psychonomic Society Inc.}},
  issn = {1069-9384},
  doi = {10.3758/BF03194105},
  url = {http://www.springerlink.com/index/10.3758/BF03194105},
  urldate = {2019-09-09},
  abstract = {The primary goal of this article is to promote awareness of the various statistical problems associated with the use of p value null-hypothesis significance testing (NHST). Making no claim of completeness, I review three prob-lems with NHST, briefly explaining their causes and con-sequences (see Karabatsos, 2006). The discussion of each problem is accompanied by concrete examples and refer-ences to the statistical literature. In the psychological literature, the pros and cons of NHST have been, and continue to be, hotly debated The issues that have dominated the NHST discussion in the psychological lit-erature are that (1) NHST tempts the user into confusing the probability of the hypothesis given the data with the probability of the data given the hypothesis; (2) (.05 is an arbitrary criterion for significance; and (3) in real-world applications, the null hypothesis is never exactly true, and will therefore always be rejected as the number of observations grows large. In the statistical literature, the pros and cons of NHST are also the topic of an ongoing dispute (e. A comparison of these two literatures shows that in psychology, the NHST discussion has focused mostly on problems of interpretation, whereas in statistics, the NHST discussion has focused mostly on problems of for-mal construction. The statistical perspective on the prob-lems associated with NHST is therefore fundamentally different from the psychological perspective. In this ar-ticle, the goal is to explain NHST and its problems from a statistical perspective. Many psychologists are oblivious to certain statistical problems associated with NHST, and the examples below show that this ignorance can have im-portant ramifications. In this article, I will show that an NHST p value de-pends on data that were never observed: The p value is a tail-area integral, and this integral is effectively over data that are not observed but only hypothesized. The prob-ability of these hypothesized data depends crucially on the possibly unknown subjective intentions of the researcher who carried out the experiment. If these intentions were to be ignored, a user of NHST could always obtain a sig-nificant result through optional stopping (i.e., analyzing the data as they accumulate and stopping the experiment whenever the p value reaches some desired significance level). In the context of NHST, it is therefore necessary to know the subjective intention with which an experi-ment was carried out. This key requirement is unattain-able in a practical sense, and arguably undesirable in a philosophical sense. In addition, I will review a proof that the NHST p value does not measure statistical evidence. In order for the p value to qualify as a measure of statisti-cal evidence, a minimum requirement is that identical p values convey identical levels of evidence, irrespective},
    keywords = {★},
  number = {5},
}
@Article{Wasserstein2016,
  title = {The {{ASA}}'s {{Statement}} on p-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  author = {Ronald L. Wasserstein and Nicole A. Lazar},
  date = {2016-04-02},
  journaltitle = {American Statistician},
  volume = {70},
  pages = {129--133},
  publisher = {{American Statistical Association}},
  issn = {15372731},
  doi = {10.1080/00031305.2016.1154108},
  annotation = {2634 citations (Semantic Scholar/DOI) [2021-02-13]},
    keywords = {★},
  number = {2},
}
@Article{wassersteinMovingWorld052019,
  title = {Moving to a {{World Beyond}} “p {$<$} 0.05”},
  author = {Ronald L. Wasserstein and Allen L. Schirm and Nicole A. Lazar},
  date = {2019-03-29},
  journaltitle = {American Statistician},
  volume = {73},
  pages = {1--19},
  publisher = {{American Statistical Association}},
  issn = {15372731},
  doi = {10.1080/00031305.2019.1583913},
  annotation = {662 citations (Semantic Scholar/DOI) [2021-02-13]},
  issue = {sup1},
}
@article{watanabe2010asymptotic,
  title={Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory.},
  author={Watanabe, Sumio and Opper, Manfred},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}
@Book{wickhamGgplot2ElegantGraphics2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Hadley Wickham},
  date = {2016},
  publisher = {{Springer-Verlag New York}},
  url = {https://ggplot2.tidyverse.org},
  isbn = {978-3-319-24277-4},
}

@book{zuur2007analyzing,
  title={Analyzing ecological data},
  author={Zuur, Alain and Ieno, Elena N and Smith, Graham M},
  year={2007},
  publisher={Springer}
}
@article{Burkner_Vuorre_2019,
  title={Ordinal Regression Models in Psychology: A Tutorial},
  volume={2},
  ISSN={2515-2459, 2515-2467},
  url={http://journals.sagepub.com/doi/10.1177/2515245918823199},
  DOI={10.1177/2515245918823199},
  number={1},
  journal={Advances in Methods and Practices in Psychological Science},
  author={Bürkner, Paul-Christian and Vuorre, Matti},
  year={2019},
  month={Mar},
  pages={77–101}
}
@misc{Semenova_2019,
  title={Ordered Logistic regression and Probabilistic Programming:with examples in Stan, PyMC3 and Turing},
  url={https://medium.com/@liza_p_semenova/ordered-logistic-regression-and-probabilistic-programming-502d8235ad3f},
  journal={Medium},
  author={Semenova, Elizaveta},
  year={2019},
  month={Aug}
}
@book{grimmettProbabilityRandomProcesses2020,
  title = {Probability and {{Random Processes}}: {{Fourth Edition}}},
  shorttitle = {Probability and {{Random Processes}}},
  author = {Grimmett, G and Stirzaker, D},
  year = {2020},
  month = sep,
  edition = {Fourth Edition, New to this Edition:},
  publisher = {{Oxford University Press}},
  address = {{Oxford, New York}},
  isbn = {978-0-19-884760-1}
}
