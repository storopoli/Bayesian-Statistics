% !TeX root = slides.tex
\section{Linear Regression}

\subsection{Recommended References}
\begin{frame}{Recommended References}
	\begin{vfilleditems}
		\item \textcite{gelman2013bayesian}:
		\begin{vfilleditems}
			\item Chapter 14: Introduction to regression models
			\item Chapter 16: Generalized linear models
		\end{vfilleditems}
		\item \textcite{mcelreath2020statistical} - Chapter 4: Geocentric Models
		\item \textcite{gelman2020regression}:
		\begin{vfilleditems}
			\item Chapter 7: Linear regression with a single predictor
			\item Chapter 8: Fitting regression models
			\item Chapter 10: Linear regression with multiple predictors
		\end{vfilleditems}
	\end{vfilleditems}
\end{frame}

\subsection{What is Linear Regression?}
\begin{frame}{What is Linear Regression?}
	The ideia here is to model a dependent variable as a linear combination
	of independent variables.
	$$
		\mathbf{y} = \alpha +  \mathbf{X} \boldsymbol{\beta} + \epsilon
	$$
	where:
	\begin{vfilleditems}
		\item $\mathbf{y}$ -- dependent variable
		\item $\alpha$ -- intercept (also called as constant)
		\item $\boldsymbol{\beta}$ -- coefficient vector
		\item $\mathbf{X}$ -- data matrix
		\item $\epsilon$ -- model error
	\end{vfilleditems}
\end{frame}

\begin{frame}{Linear Regression Specification}
	To estimate the intercept $\alpha$ and coefficients $\boldsymbol{\beta}$
	we use a Gaussian/normal likelihood function.
	Mathematically speaking, Bayesian linear regression is:
	$$
		\begin{aligned}
			\mathbf{y}         & \sim \text{Normal}\left( \alpha +  \mathbf{X} \boldsymbol{\beta}, \sigma \right) \\
			\alpha             & \sim \text{Normal}(\mu_\alpha, \sigma_\alpha)                                    \\
			\boldsymbol{\beta} & \sim \text{Normal}(\mu_{\boldsymbol{\beta}}, \sigma_{\boldsymbol{\beta}})        \\
			\sigma             & \sim \text{Exponential}(\lambda_\sigma)
		\end{aligned}
	$$
\end{frame}

\begin{frame}{Linear Regression Specification}
	What we are missing is the prior probabilities for the model's parameters:
	\begin{vfilleditems}
		\item Prior Distribution for $\alpha$ --
		Knowledge that we have about the model's intercept.
		\item Prior Distribution for $\boldsymbol{\beta}$ --
		Knowledge that we have about the model's independent variable coefficients.
		\item Prior Distribution for $\sigma$ --
		Knowledge that we have about the model's error.
	\end{vfilleditems}
\end{frame}

\subsubsection{Good Candidates for Prior Distribution}
\begin{frame}{Good Candidates for Prior Distribution}
	First, center ($\mu = 0$) and standardize ($\sigma = 1$) the independent variables.
	\begin{vfilleditems}
		\item $\alpha$ -- either a normal or student-$t$ ($\nu = 3$), with mean as $\mu_{\mathbf{y}}$
		and standard deviation as $2.5 \cdot \sigma_{\mathbf{y}}$
		(also you can use the median and median absolute deviation).
		\item $\boldsymbol{\beta}$ -- either a normal or student-$t$ ($\nu = 3$), with mean $0$ and
		standard deviation $2.5$.
		\item $\sigma$ -- anything that is long-tailed (mass towards lower values)
		and restrained to positive values only.
		Exponential is a good candidate.
	\end{vfilleditems}
\end{frame}

\subsubsection{Posterior Computation}
\begin{frame}{Posterior Computation}
	Our aim to is to \textbf{find the posterior distribution of the
		model's parameters of interest} ($\alpha$ and $\boldsymbol{\beta}$)
	by computing the full posterior distribution of:
	$$
		P(\boldsymbol{\theta} \mid \mathbf{y}) = P(\alpha, \boldsymbol{\beta}, \sigma \mid \mathbf{y})
	$$
\end{frame}
