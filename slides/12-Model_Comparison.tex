\section{Model Comparison}

\subsection{Model Comparison - Recommended References}
\begin{frame}{Model Comparison - Recommended References}
	\begin{vfilleditems}
		\item \textcite{gelman2013bayesian} - Chapter 7: Evaluating, comparing, and expanding models
		\item \textcite{gelman2020regression} - Chapter 11, Section 11.8: Cross validation
		\item \textcite{mcelreath2020statistical} - Chapter 7, Section 7.5: Model comparison
		\item \textcite{vehtariPracticalBayesianModel2015}
		\item \textcite{spiegelhalter2002bayesian}
		\item \textcite{van2005dic}
		\item \textcite{watanabe2010asymptotic}
		\item \textcite{gelfand1996model}
		\item \textcite{watanabe2010asymptotic}
		\item \textcite{geisser1979predictive}
	\end{vfilleditems}
\end{frame}

\subsection{Why Compare Models?}
\begin{frame}{Why Compare Models?}
	After model parameters estimation,
	many times we want to measure its \textbf{predictive accuracy} by itself,
	or for \textbf{model comparison}, \textbf{model selection}, or computing a \textbf{model performance metric}
	\parencite{geisser1979predictive}.
\end{frame}

\begin{frame}{But What About Posterior Predictive Checks?}
	To analyze and compare models using predictive accuracy is a
	\textbf{subjective and arbitrary approach}.
	\vfill
	There is an \textbf{objective approach to compare Bayesian models}
	which uses a robust metric that helps us
	select the best model in a set of candidate models.
	\vfill
	Having an objective way of comparing and choosing the best model is very important.
	In the \textbf{Bayesian workflow},
	we generally have several iterations between priors and likelihood functions
	resulting in several different models \parencite{gelmanBayesianWorkflow2020}.
\end{frame}

\subsection{Model Comparison Techniques}
\begin{frame}{Model Comparison Techniques}
	We have several model comparison techniques that use \textbf{predictive accuracy},
	but the main ones are:
	\begin{vfilleditems}
		\item Leave-one-out cross-validation (LOO)
		\parencite{vehtariPracticalBayesianModel2015}.
		\item Deviance Information Criterion (DIC)
		\parencite{spiegelhalter2002bayesian},
		but it is known to have some issues,
		due to not being full-Bayesian,
		because it is only based on point estimates \parencite{van2005dic},
		\item Widely Applicable Information Criteria (WAIC) \parencite{watanabe2010asymptotic},
		full-Bayesian,
		in the sense that uses the full posterior distribution density,
		and it is asymptotically equal to LOO \parencite{vehtariPracticalBayesianModel2015}.
	\end{vfilleditems}
\end{frame}

\begin{frame}{Historical Interlude}
	\small
	In the past, we did not have computational power and data abundance.
	Model comparison was done based on a theoretical divergence metric
	originated from information theory's entropy:
	$$
		H(p) = - \operatorname{E}\log(p_i) = -\sum^N_{i=1} p_i \log(p_i)
	$$
	\small
	We compute the divergence by multiplying entropy by $-2$\footnote{historical reasons.},
	so lower values are preferable:
	$$
		D(y, \boldsymbol{\theta}) = -2 \cdot \underbrace{\sum^N_{i=1} \log \frac{1}{S}\sum^S_{s=1} P(y_i \mid \boldsymbol{\theta}^s)}_{\text{\textit{log pointwise predictive density} -- lppd}}
	$$
	\footnotesize
	where $n$ is the sample size and $S$ is the number of posterior draws.
\end{frame}

\begin{frame}{Historial Interlude -- AIC \parencite{akaike1998information}}
	$$
		\text{AIC} = D(y, \boldsymbol{\theta}) + 2k = -2 \text{lppd}_{\text{mle}} + 2k
	$$
	where $k$ is the number of the model's free parameters and
	$\text{lppd}_{\text{mle}}$ is the
	\textbf{m}aximum \textbf{l}ikelihood \textbf{e}stimate of the
	\textbf{l}og \textbf{p}ointwise \textbf{p}redictive \textbf{d}ensity.
	\vfill
	AIC is an approximation and can only be reliable when:
	\begin{vfilleditems}
		\item The priors are uniform (flat priors) or totally dominated by the likelihood function.
		\item The posterior is approximate a multivariate normal distribution.
		\item The sample size $N$ is much larger than the number of the model's free parameters $k$: $N \gg k$
	\end{vfilleditems}
\end{frame}

\begin{frame}{Historical Interlude -- DIC \parencite{spiegelhalter2002bayesian}}
	A generalization of the AIC,
	where we replace the maximum likelihood estimate for the posterior mean
	and $k$ by a data-based bias correction:
	$$
		\text{DIC} = D(y, \boldsymbol{\theta}) + k_{\text{DIC}} = -2 \text{lppd}_{\text{Bayes}}
		+2 \underbrace{\left( \text{lppd}_{\text{Bayes}} - \frac{1}{S} \sum^S_{s=1} \log P(y \mid \boldsymbol{\theta}^s) \right)}_{\text{bias-corrected $k$}}
	$$
	DIC removes the restriction on uniform AIC priors, but still
	keeps the assumptions of the posterior being a multivariate Gaussian/normal distribution
	and that $N \gg k$.
\end{frame}

\subsubsection{Predictive Accuracy}
\begin{frame}{Predictive Accuracy}
	With current computational power, we do not need approximations\footnote{AIC, DIC etc.}.
	\vfill
	We can discuss \textbf{predictive accuracy objective metrics}
	\vfill
	But, first, let's define what is predictive accuracy.
\end{frame}

\begin{frame}{Predictive Accuracy}
	\begin{defn}[Predictive Accuracy]
		Bayesian approaches measure predictive accuracy using posterior draws
		$\tilde{y}$ from the model.
		For that we have the predictive posterior distribution:
		$$
			p(\tilde{y} \mid y) = \int p(\tilde{y}_i \mid \theta) p(\theta \mid y) d \theta
		$$
		\small
		Where $p(\theta \mid y)$ is the model's posterior distribution.
		The above equation means that we evaluate the integral
		with respect to the whole joint probability of the model's
		predictive posterior distribution and posterior distribution.
		\vfill
		\normalsize
		The \textbf{higher} the predictive posterior distribution
		$p(\tilde{y} \mid y)$,
		the \textbf{better} will be the model's predictive accuracy.
	\end{defn}
\end{frame}

\begin{frame}{Predictive Accuracy}
	To make samples comparable,
	we calculate the expectation of this measure for each one of the $N$ sample observations:

	$$
		\operatorname{elpd} = \sum_{i=1}^N \int p_t(\tilde{y}_i) \log p(\tilde{y}_i \mid y) d \tilde{y}
	$$
	where $\operatorname{elpd}$ is the \textbf{e}xpected \textbf{l}og \textbf{p}ointwise \textbf{p}redictive \textbf{d}ensity,
	and $p_t(\tilde{y}_i)$ is the distribution that represents the
	$\tilde{y}_i$'s true underlying data generating process.
	The $p_t(\tilde{y}_i)$ are unknown and we generally use
	cross-validation or approximations to estimate $\operatorname{elpd}$.
\end{frame}

\subsubsection{Leave-One-Out Cross-Validation (LOO)}
\begin{frame}{Leave-One-Out Cross-Validation (LOO)}
	We can compute the $\operatorname{elpd}$ using LOO
	\parencite{vehtariPracticalBayesianModel2015}:
	$$
		\operatorname{elpd}_{\text{loo}} = \sum_{i=1}^N \log p(y_i \mid y_{-i})
	$$
	where
	$$
		p(y_i \mid y_{-i}) = \int p(y_i \mid \theta) p(\theta \mid y_{-i}) d \theta
	$$
	which is the predictive density conditioned on the data without a single observation $i$ ($y_{-i}$).
	Almost always we use the PSIS-LOO\footnote{upcoming...} approximation
	due to its robustness and low computational cost.
\end{frame}

\subsubsection{Widely Applicable Information Criteria (WAIC)}
\begin{frame}{Widely Applicable Information Criteria (WAIC)}
	\footnotesize
	WAIC \parencite{watanabe2010asymptotic}, like LOO,
	is also an alternative approach to compute the $\operatorname{elpd}$,
	and is defined as:
	$$
		\widehat{\operatorname{elpd}}_{\text{waic}} = \widehat{\operatorname{lppd}} - \widehat{p}_{\text{waic}}
	$$
	where $\widehat{p}_{\text{waic}}$ is the number of effective parameters based on:
	$$
		\widehat{p}_{\text{waic}} = \sum_{i=1}^N \operatorname{var}_{\text{post}} (\log p(y_i \mid \theta))
	$$
	which we can compute using the posterior variance of the log predictive density for each observation $y_i$:
	$$
		\widehat{p}_{\text{waic}} = \sum_{i=1}^N V^S_{s=1} (\log p(y_i \mid \theta^s))
	$$
	where $V^S_{s=1}$ is the sample's variance:
	$$
		V^S_{s=1} a_s = \frac{1}{S-1} \sum^S_{s=1} (a_s - \bar{a})^2
	$$
\end{frame}

\subsubsection{$K$-fold Cross-Validation ($K$-fold CV)}
\begin{frame}{$K$-fold Cross-Validation ($K$-fold CV)}
	In the same manner that we can compute the $\operatorname{elpd}$
	using LOO with $N-1$ sample partitions,
	we can also compute it with any desired partition number.
	\vfill
	Such approach is called \textbf{$K$-fold cross-validation} ($K$-fold CV).
	\vfill
	Contrary to LOO, we cannot approximate the actual $\operatorname{elpd}$
	using $K$-fold CV,
	and we need to compute the actual $\operatorname{elpd}$ over $K$ partitions,
	which almost involves a \textbf{high computational cost}.
\end{frame}

\subsection{Pareto Smoothed Importance Sampling LOO (PSIS-LOO)}
\begin{frame}{Pareto Smoothed Importance Sampling LOO (PSIS-LOO)}
	PSIS uses \textbf{importance sampling}\footnote{another class of MCMC algorithm that we did not cover yet.},
	which means a importance weighting scheme approach.
	\vfill
	The \textbf{Pareto smoothing} is a technique to increase the importance weights' reliability.
\end{frame}

\begin{frame}{Importance Sampling}
	If the $N$ samples are conditionally independent\footnote{
		that is, they are independent if conditioned on the model's parameters,
		which is a basic assumption in any Bayesian (and frequentist) model}
	\parencite{gelfand1992model},
	we can compute LOO with $\boldsymbol{\theta}^s$ posterior' samples
	$P(\theta \mid y)$ using \textbf{importance weights}:
	$$
		r_i^s=\frac{1}{P(y_i|\theta^s)} \propto \frac{P(\theta^s|y_{-i})}{P(\theta^s|y)}
	$$
	Hence, to get Importance Sampling Leave-One-Out (IS-LOO):
	$$
		P(\tilde{y}_i|y_{-i})
		\approx
		\frac{\sum_{s=1}^S r_i^s P(\tilde{y}_i|\theta^s)}{\sum_{s=1}^S r_i^s}
	$$
\end{frame}

\begin{frame}{Importance Sampling}
	However, the posterior $P(\theta \mid y)$ often has low variance and shorter tails
	than the LOO distributions $P(\theta \mid y_{-1})$.
	Hence, if we use:
	$$
		P(\tilde{y}_i|y_{-i}) \approx \frac{\sum_{s=1}^S r_i^s P(\tilde{y}_i|\theta^s)}{\sum_{s=1}^S r_i^s}
	$$
	we will have \textbf{instabilities} because the $r_i$ can have \textbf{high, or even infinite, variance}.
\end{frame}

\begin{frame}{Pareto Smoothed Importance Sampling}
	We can enhance the IS-LOO estimate using a \textbf{Pareto Smoothed Importance Sampling}
	\parencite{vehtariPracticalBayesianModel2015}.
	\vfill
	When the tails of the importance weights' distribution are long,
	a direct usage of the importance is sensible to one or more large value.
	By \textbf{fitting a generalized Pareto distribution to the importance weights' upper-tail},
	we smooth out these values.
\end{frame}

\begin{frame}{Pareto Smoothed Importance Sampling LOO (PSIS-LOO)}
	Finally, we have PSIS-LOO:
	$$
		\widehat{\operatorname{elpd}}_{\rm psis-loo} =
		\sum_{i=1}^n \log
		\left(\frac{\sum_{s=1}^S w_i^s P(y_i|\theta^s)}{\sum_{s=1}^Sw_i^s} \right)
	$$
	where $w$ is the truncated weights.
\end{frame}

\begin{frame}{Pareto Smoothed Importance Sampling LOO (PSIS-LOO)}
	\small
	We use the importance weights Pareto distribution's estimated shape parameter
	$\widehat{k}$ to assess its reliability:
	\begin{vfilleditems}
		\item \small $k < \frac{1}{2}$: the importance weights variance is finite,
		the central limit theorem holds, and the estimate rapidly converges.
		\item \small $\frac{1}{2} < k < 1$ the importance weights variance is infinite,
		but the mean exists (is finite),
		the generalized central limit theorem for stable distributions holds,
		and the estimate converges, but slower.
		The PSIS variance estimate is finite, but could be large.
		\item \small $k > 1$ both the importance weights variance and mean do not exist
		(they are infinite).
		The PSIS variance estimate is finite, but could be large.
	\end{vfilleditems}
	\vfill
	\small
	Any $\widehat{k} > 0.5$ is a warning sign,
	but empirically there is still a good performance up to $\widehat{k} < 0.7$.
\end{frame}
